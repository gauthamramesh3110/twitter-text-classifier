{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Politician Rating Prediction by Analysing Twitter Data\n",
    "\n",
    "We start by importing the NLTK package and the twitter_samples dataset from the NLTK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The twitter_samples dataset contains 3 json file: \n",
    "1. negative_tweets.json \n",
    "2. positive_tweets.json\n",
    "3. tweets.20150430-223406.json\n",
    "\n",
    "As the name suggests, the first file contains negative tweets, the second contains positive tweets, and the third contains test data to test its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n"
     ]
    }
   ],
   "source": [
    "fileids = [fileid for fileid in twitter_samples.fileids()]\n",
    "print(fileids[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of the data is show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hopeless for tmr :(\n",
      "Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
      "@Hegelbon That heart sliding into the waste basket. :(\n",
      "“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\n",
      "\n",
      "Me too\n",
      "Dang starting next week I have \"work\" :(\n",
      "oh god, my babies' faces :( https://t.co/9fcwGvaki0\n",
      "@RileyMcDonough make me smile :((\n",
      "@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln\n",
      "why?:(\"@tahuodyy: sialan:( https://t.co/Hv1i0xcrL2\"\n",
      "Athabasca glacier was there in #1948 :-( #athabasca #glacier #jasper #jaspernationalpark #alberta #explorealberta #… http://t.co/dZZdqmf7Cz\n"
     ]
    }
   ],
   "source": [
    "strings = twitter_samples.strings(fileids[0])\n",
    "\n",
    "for string in strings[:10]:\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As you can see, the data contains tweets by real users, and the above tweets are from the <em>negative_tweets.json</em> file. It requires quite a bit of preprocessing to make it ready and trainable. This requires a few steps:\n",
    "\n",
    "1. Remove the twitter handle mentions (as they don't serve purpose in determining whether a tweet is negative or positive and may lead to bias).\n",
    "2. Word Tokenize the sentences.\n",
    "3. Part of speech (pos) tagging for easier lemmatization\n",
    "4. Lemmatization using the correct pos-tagging\n",
    "5. Filter stop words to reduce the dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>remove_handle</b> functions uses a regular expression to identify twitter handle mentions (ex: @TheRealDonald) and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_handle(string):\n",
    "    regex = r\"(^|[^@\\w])@(\\w{1,15})\\b\"\n",
    "    return re.sub(regex, '', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>get_wordnet_pos</b> function returns the correct Wordnet POS object for the given POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(pos):\n",
    "    if pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The main processing functions is given below:\n",
    "\n",
    "The first step is to import the required packages. The <em> stopwords</em> package contains a corpus of stopwords. Stop words are words that don't particularly add meaning and is useful only for human communication and does not help computer understand meaning.\n",
    "\n",
    "We use a special kind of tokenizer called TweetTokenizer, which can identify emoticons and group them together instead of separating the characters like a standard word_tokenizer would do. This helps in maintaining the emoticons which can give an insight into whether a tweet is negative or positive.\n",
    "\n",
    "The WordNetLemmatizer is a powerful lemmatizer. A lemmatizer reduces a given word to its root word. <br>\n",
    "<em>Example: Better -> Good and Eating->Eat </em> <br> This can prevent bias and improve reliability.\n",
    "\n",
    "The <b>process_string</b> function performs the 5 steps mentioned previously to each string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def process_string(string):\n",
    "    string = remove_handle(string) #remove twitter handle\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    word_tokens = tokenizer.tokenize(string)#tokenize the sentence\n",
    "    \n",
    "    tagged = nltk.pos_tag(word_tokens)#pos tagging\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(pos)) for (w , pos) in tagged]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] #filter stop words\n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>process_data</b> functions processes the whole document of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_strings):\n",
    "    processed_data = []\n",
    "    \n",
    "    for string in data_strings:\n",
    "        processed_data.append(process_string(string))\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code processes every tweet from both <em>negative_tweets.json</em> and <em>positive_tweets.json</em>, and adds the class for each tweet (pos/neg). Then all the data is combined together and shuffled. Shuffling has to be done so that any chance of overfitting can be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "positive_data \n",
    "\n",
    "for fileid in fileids[:2]:\n",
    "    strings = twitter_samples.strings(fileid)\n",
    "    \n",
    "    p_s = process_data(strings)\n",
    "    for p in p_s:\n",
    "        processed_data.append((p, fileid[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['dude', ':(', \"I'm\", 'scared', 'lol'], 'neg')\n",
      "(['Happy', 'Friday', ':)'], 'pos')\n",
      "(['I', 'shoulda', 'moved', 'away', 'w', 'boys', 'I', 'chance', 'cause', \"they're\", 'people', 'I', 'want', 'times', 'like', ':('], 'neg')\n",
      "(['double', 'whammy', ':('], 'neg')\n",
      "(['Great', ',', 'I', 'went', 'Kardamena', ',', 'last', 'summer', ',', 'amazing', 'beaches', 'wonderful', 'sea', ',', 'year', \"It's\", 'time', 'go', 'Samos', '#Greece', ':)', ':)', '!', '!'], 'pos')\n",
      "(['#Midlands', 'Yes', 'thanks', 'depressing', 'weather', 'forecast', ',', 'word', \"'\", 'rain', \"'\", 'mentioned', 'several', 'times', ':-('], 'neg')\n",
      "(['Hey', 'Fam', '!', 'Vote', 'already', ':)', 'WE', 'GOTTA', 'WIN', 'THIS', 'FOR', 'OUR', 'BOYS', '!', '#TeenChoice', '-', 'S', 'http://t.co/zBZvXC0v5y'], 'pos')\n",
      "(['made', 'stuff', 'tonight', 'streamer', ':)', 'felt', 'really', 'nice', 'getting', 'creative', 'juices', 'flowing', '.', 'havent', 'done'], 'pos')\n",
      "(['Enjoy', '!', 'And', 'sorry', 'hastily', 'typed', 'message', '!', ':)'], 'pos')\n",
      "(['ok', '...', 'sering', '2', 'play', 'yah', 'min', '..', 'HAHA', ':)', ')'], 'pos')\n"
     ]
    }
   ],
   "source": [
    "for data in processed_data[:10]:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now we are in the training part. We are going to use the Naive Bayes classifier, which is a binary classifier. This functions is built in to the nltk package. <br>\n",
    "\n",
    "<em> Before we start training, we need convert the processed data into a frequency distribuition and split the data in training / testing sets. </em>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dude', ':(', \"I'm\", 'scared', 'lol', 'Happy', 'Friday', ':)', 'I', 'shoulda']\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "\n",
    "for (string, key) in processed_data:\n",
    "    for word in string:\n",
    "        all_words.append(word)\n",
    "print(all_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = [w[0] for w in all_words.most_common(3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':(', ':)', '!', 'I', '.', ',', '(', '?', ':-)', ':D']\n"
     ]
    }
   ],
   "source": [
    "print(word_features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The find_features() function:\n",
    "\n",
    "This functions checks our word_features with all the features in our data set, and mark whether that feature is present or not through a boolean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(tweet), category) for (tweet, category) in processed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = featuresets[:7000]\n",
    "testing_set = featuresets[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training part:\n",
    "\n",
    "Finally after preprocessing the data, we are now ready to train. Its a simple one-liner, and we can see that we have a high accuracy of <b>99.8%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 99.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twitter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-3b7b9df30395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# initialize api instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m twitter_api = twitter.Api(consumer_key='YOUR_CONSUMER_KEY',\n\u001b[1;32m      5\u001b[0m                         \u001b[0mconsumer_secret\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'YOUR_CONSUMER_SECRET'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'twitter'"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "\n",
    "# initialize api instance\n",
    "twitter_api = twitter.Api(consumer_key='YOUR_CONSUMER_KEY',\n",
    "                        consumer_secret='YOUR_CONSUMER_SECRET',\n",
    "                        access_token_key='YOUR_ACCESS_TOKEN_KEY',\n",
    "                        access_token_secret='YOUR_ACCESS_TOKEN_SECRET')\n",
    "\n",
    "# test authentication\n",
    "print(twitter_api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
